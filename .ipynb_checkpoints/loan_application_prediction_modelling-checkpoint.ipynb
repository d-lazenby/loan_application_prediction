{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "df = pd.read_csv('Loan-Approval-Prediction.csv')\n",
    "\n",
    "categorical = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', \n",
    "               'Property_Area']\n",
    "\n",
    "numerical = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe145f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047dbc23",
   "metadata": {},
   "source": [
    "### Split into training and test sets\n",
    "\n",
    "Ideally, to avoid data leakage, no data from the test set should be used in preprocessing, so we'll split into training and test sets beforehand. Given that we have less than 1000 rows, we'll have a relatively small test set and make an 80:20 split. We can also drop the loan ID since this won't be used in modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate target out and remove loan ID column\n",
    "X = df.iloc[:, 1:].copy()\n",
    "y = X.pop('Loan_Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94594185",
   "metadata": {},
   "source": [
    "We encode the target as a binary, applying the convention that the minority class is labelled as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda x: 1 if x=='N' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a08aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shapes of splits\n",
    "[split.shape for split in [X_train, X_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ef266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check class distribution in target splits\n",
    "print(y_train.value_counts(normalize=True), y_test.value_counts(normalize=True), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b96970",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "\n",
    "- For the categorical columns (including the loan term), we'll replace all nulls with the most frequent value. Given that 8% of the credit history values are null, this isn't ideal, but they mostly (~84%) take the value 1, so this seems reasonable in this case.\n",
    "- Since the other numerical columns are right-skewed, we'll replace the nulls with the median rather than the mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1116c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check nulls\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1288064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values with median\n",
    "value = {'LoanAmount': X_train.loc[:, ('LoanAmount')].median()}\n",
    "X_train = X_train.fillna(value=value)\n",
    "X_test = X_test.fillna(value=value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values with most frequent value\n",
    "for col in categorical + ['Loan_Amount_Term']:\n",
    "    value = {col: X_train.loc[:, (col)].value_counts().index[0]}\n",
    "    X_train = X_train.fillna(value=value)\n",
    "    X_test = X_test.fillna(value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f53474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls in data\n",
    "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cf51b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First we'll recast our datatypes where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83933be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in X_train, X_test:\n",
    "    data = data.astype({'CoapplicantIncome': 'int64', \n",
    "                        'LoanAmount': 'int64', \n",
    "                        'Loan_Amount_Term': 'int64', \n",
    "                        'Credit_History': 'int64'})\n",
    "    print(data.dtypes, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cf3c5",
   "metadata": {},
   "source": [
    "We note the following with regard to encoding each feature. \n",
    "- Gender, Married, Education, Self_Employed and Credit_History are binary, so we'll one-hot encode these. Property_Area takes three values but there doesn't appear to be any inherent ordering so we'll also one-hot encode this and drop a redundant column.\n",
    "- Dependents takes string values 0, 1, 2 and 3+. There is an ordering here so we could cast these as integers 0, 1, 2, 3, respectively, but note that this doesn't take into account the fact that a label 3 could then indicate any number of dependents from 3 upwards, while the other labels exactly match the data. Given this, we'll simply one-hot encode these as well, dropping a redundant column, as is standard.\n",
    "- ApplicantIncome, CoapplicantIncome, LoanAmount and Loan_Amount_Term are all numeric and take only positive values, so we'll scale these using a MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [('categorical', OneHotEncoder(sparse=True, handle_unknown='ignore'), categorical), \n",
    "                ('numerical', MinMaxScaler(feature_range=(0,1)), numerical)]\n",
    "\n",
    "col_transform = ColumnTransformer(transformers=transformers, remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61c822",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "We now test a suite of classifiers with default parameters using repeated stratified cross-validation with three repeats and three folds. We monitor both accuracy and F1 score to ensure that we have a good balance between overall accuracy, recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cda507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, fbeta_score, make_scorer, f1_score, average_precision_score\n",
    "\n",
    "#Define metrics\n",
    "accs = make_scorer(accuracy_score)\n",
    "f1 = make_scorer(fbeta_score, beta=1)\n",
    "aps = make_scorer(average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = [\n",
    "    ('LR', LogisticRegression(solver='lbfgs', max_iter=1500)),\n",
    "    ('SVC', SVC(gamma='scale', max_iter=1500)),\n",
    "    ('DT', DecisionTreeClassifier(max_depth=3)),\n",
    "    ('RF', RandomForestClassifier(max_depth=3)),\n",
    "    ('XGB', XGBClassifier(max_depth=3))\n",
    "]\n",
    "\n",
    "# define cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=21)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "#     define data prep and pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessing', col_transform), ('modelling', classifier)])\n",
    "\n",
    "#     evaluate pipeline using cross validation \n",
    "    scores = cross_validate(pipeline, X_train, y_train, \n",
    "                            scoring={'accuracy': accs, 'f1': f1, 'APS': aps}, \n",
    "                            cv=cv, n_jobs=-1, error_score='raise')\n",
    "    result = pd.DataFrame(scores)\n",
    "    result['model'] = name\n",
    "    results.append(result)\n",
    "    \n",
    "    #In case we need them later\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     predictions = pipeline.predict(X_train)\n",
    "#     yhat = pipeline.predict(X_test)\n",
    "\n",
    "final = pd.concat(results, ignore_index=True)\n",
    "time_metrics = [col for col in final.columns if 'time' in col]\n",
    "test_metrics = [col for col in final.columns if 'test' in col]\n",
    "\n",
    "#Plot results\n",
    "final_test = pd.melt(final, id_vars=['model'], value_vars=test_metrics,\n",
    "                     var_name='metric')\n",
    "final_times = pd.melt(final, id_vars=['model'], value_vars=time_metrics,\n",
    "                     var_name='metric')\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "\n",
    "sns.boxplot(x='model', y='value', data=final_test, hue='metric', palette='Set2', ax=ax[0])\n",
    "sns.boxplot(x='model', y='value', data=final_times, hue='metric', palette='Set3', ax=ax[1])\n",
    "\n",
    "ax[0].set_ylabel('Values')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "ax[0].set_title('Performance by model and metric')\n",
    "\n",
    "ax[1].set_ylabel('Time')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "ax[1].set_title('Performance by model and fit/score time')\n",
    "\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc21be",
   "metadata": {},
   "source": [
    "The models perform similarly in overall accuracy at about 80% (compared to a baseline of 69%) and it's difficult to distinguish visually which might be the best model, so we'll take a closer look at the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65550854",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unbalanced = final_test.groupby(['metric', 'model']).median()\n",
    "results_unbalanced.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f80ee1",
   "metadata": {},
   "source": [
    "It's now easier to see that the random forest, logistic regression and SVC models perform equivalently, and in fact have exactly the same scores on some of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049c897",
   "metadata": {},
   "source": [
    "Let's rerun the experiments with class weights balanced to see if the class imbalance is having a negative effect. For the XGBoost classifier, a good starting value for scale_pos_weight is the ratio of negative to positive examples in the dataset, which in our case is given as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()[0] / y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define balanced classifiers\n",
    "classifiers = [\n",
    "    ('LR', LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=1000)),\n",
    "    ('SVC', SVC(gamma='scale', class_weight='balanced', max_iter=1000)),\n",
    "    ('DT', DecisionTreeClassifier(class_weight='balanced', max_depth=3)),\n",
    "    ('RF', RandomForestClassifier(class_weight='balanced', max_depth=3)),\n",
    "    ('XGB', XGBClassifier(scale_pos_weight=2.19, max_depth=3))\n",
    "]\n",
    "\n",
    "\n",
    "# define cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "#     define data prep and pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessing', col_transform), ('modelling', classifier)])\n",
    "\n",
    "#     evaluate pipeline using cross validation \n",
    "    scores = cross_validate(pipeline, X_train, y_train, \n",
    "                            scoring={'accuracy': accs, 'f1': f1, 'APS': aps}, \n",
    "                            cv=cv, n_jobs=-1, error_score='raise')\n",
    "    result = pd.DataFrame(scores)\n",
    "    result['model'] = name\n",
    "    results.append(result)\n",
    "    \n",
    "    #In case we need them later\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     predictions = pipeline.predict(X_train)\n",
    "#     yhat = pipeline.predict(X_test)\n",
    "\n",
    "final = pd.concat(results, ignore_index=True)\n",
    "time_metrics = [col for col in final.columns if 'time' in col]\n",
    "test_metrics = [col for col in final.columns if 'test' in col]\n",
    "\n",
    "#Plot results\n",
    "final_test = pd.melt(final, id_vars=['model'], value_vars=test_metrics,\n",
    "                     var_name='metric')\n",
    "final_times = pd.melt(final, id_vars=['model'], value_vars=time_metrics,\n",
    "                     var_name='metric')\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "\n",
    "sns.boxplot(x='model', y='value', data=final_test, hue='metric', palette='Set2', ax=ax[0])\n",
    "sns.boxplot(x='model', y='value', data=final_times, hue='metric', palette='Set3', ax=ax[1])\n",
    "\n",
    "ax[0].set_ylabel('Values')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "ax[0].set_title('Performance by model and metric')\n",
    "\n",
    "ax[1].set_ylabel('Time')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "ax[1].set_title('Performance by model and fit/score time')\n",
    "\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_balanced = final_test.groupby(['metric', 'model']).median()\n",
    "results_balanced.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06a0d7",
   "metadata": {},
   "source": [
    "We note that the cost-sensitive versions have a decrease in overall accuracy except for random forest, which is about the same. However, we should check the confusion matrices to evaluate how recall and precision vary for each.\n",
    "\n",
    "We'll do this for the logistic regression, random forest and SVC models, since these all performed equivalently, at least in the non-cost-sensitive versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb00b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('LogisticRegression', LogisticRegression(solver='lbfgs', max_iter=1500)), \n",
    "          ('SVC', SVC(gamma='scale', max_iter=1500)),\n",
    "          ('RandomForest', RandomForestClassifier(max_depth=3))]\n",
    "\n",
    "for title, model in models:\n",
    "    pipe = Pipeline(steps=[('preprocessing', col_transform), (title.lower(), model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    #Get predictions\n",
    "    yhat = pipe.predict(X_test)\n",
    "    \n",
    "    acc = round(pipe.score(X_test, y_test), 3)\n",
    "    fscore = round(f1_score(y_test, yhat), 3)\n",
    "    apscore = round(average_precision_score(y_test, yhat), 3)\n",
    "    \n",
    "    print(title + '\\n', classification_report(y_test, yhat, labels=[1,0]), \"\\n\")\n",
    "    \n",
    "    #Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test, yhat, labels=[1,0]), display_labels=[1,0])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'{title}\\n Accuracy: {acc}; F1: {fscore}; APS: {apscore}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486a172",
   "metadata": {},
   "source": [
    "Interestingly, all three perform perform exactly the same, which is perhaps unsurprising given that the accuracy and F1 scores from cross-validation are the same in all three cases. We also note that all scores decrease from the training stage, which makes sense, and that recall is poor and precision is perfect (there are 25 false negatives and no false positives), i.e. the models are good at predicting loans that will be approved (the majority class) and poor at predicting if they will be rejected. Given that it is often more important to identify  whether loans will be rejected than when they will be accepted, we can assume that the recall could be improved at the expense of some precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_lr = Pipeline(steps=[('preprocessing', col_transform), ('LR', models[0][1])]).predict(X_test)\n",
    "yhat_svc = Pipeline(steps=[('preprocessing', col_transform), ('SVC', models[1][1])]).predict(X_test)\n",
    "yhat_rf = Pipeline(steps=[('preprocessing', col_transform), ('RF', models[2][1])]).predict(X_test)\n",
    "\n",
    "print(sum(yhat_lr == yhat_svc) == len(X_test), sum(yhat_lr == yhat_rf) == len(X_test), \n",
    "      sum(yhat_rf == yhat_svc) == len(X_test), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348b961",
   "metadata": {},
   "source": [
    "So the models made exactly the same predictions.\n",
    "\n",
    "Given that they perform the same and the dataset is small enough that computation time isn't an issue, we'll gridsearch all three to obtain the optimum hyperparameters. We should also check the cost-sensitive versions to see if recall is any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('LogisticRegression', LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=1500)), \n",
    "          ('SVC', SVC(class_weight='balanced', gamma='scale', max_iter=1500)),\n",
    "          ('RandomForest', RandomForestClassifier(class_weight='balanced', max_depth=3))]\n",
    "\n",
    "for title, model in models:\n",
    "    pipe = Pipeline(steps=[('preprocessing', col_transform), (title.lower(), model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    #Get predictions\n",
    "    yhat = pipe.predict(X_test)\n",
    "    \n",
    "    acc = round(pipe.score(X_test, y_test), 3)\n",
    "    fscore = round(f1_score(y_test, yhat), 3)\n",
    "    apscore = round(average_precision_score(y_test, yhat), 3)\n",
    "    \n",
    "    print(title + '\\n', classification_report(y_test, yhat, labels=[1,0]), \"\\n\")\n",
    "    \n",
    "    #Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test, yhat, labels=[1,0]), display_labels=[1,0])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'{title}\\n Accuracy: {acc}; F1: {fscore}; APS: {apscore}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eda57",
   "metadata": {},
   "source": [
    "Beforehand, we'll explore some feature engineering to see if we can improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0124b",
   "metadata": {},
   "source": [
    "### Power transforms of numerical features\n",
    "\n",
    "Some ML algorithms (e.g. logistic regression) assume that the input data is approximately normally distributed; if it is not this can negatively affect performance. We can make numerical distributions more normally distributed by applying a power transform in the preprocessing step. By default, this also includes standardization after the transform is applied but we'll forgo this and apply a MinMaxScaler() for consistency with our previous process. Note that we have omitted the loan term from this process and simply scaled it since it is a kind of hybrid categorical/numerical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical transforms\n",
    "num_transformer = Pipeline(steps=[('power', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "                                  ('scaler', MinMaxScaler(feature_range=(0,1)))])\n",
    "\n",
    "#categorical transforms\n",
    "cat_transformer = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
    "\n",
    "#transformer\n",
    "col_transform = ColumnTransformer(transformers=[('categorical', cat_transformer, categorical),\n",
    "                                                ('numerical', num_transformer, numerical[:-1]),\n",
    "                                                ('scaler', MinMaxScaler(feature_range=(0,1)), [numerical[-1]])],\n",
    "                                  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d87244",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('LogisticRegression', LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=1500)), \n",
    "          ('SVC', SVC(class_weight='balanced', gamma='scale', max_iter=1500)),\n",
    "          ('RandomForest', RandomForestClassifier(class_weight='balanced', max_depth=3))]\n",
    "\n",
    "for title, model in models:\n",
    "    pipe = Pipeline(steps=[('preprocessing', col_transform), (title.lower(), model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    #Get predictions\n",
    "    yhat = pipe.predict(X_test)\n",
    "    \n",
    "    acc = round(pipe.score(X_test, y_test), 3)\n",
    "    fscore = round(f1_score(y_test, yhat), 3)\n",
    "    apscore = round(average_precision_score(y_test, yhat), 3)\n",
    "    \n",
    "    print(title + '\\n', classification_report(y_test, yhat, labels=[1,0]), \"\\n\")\n",
    "    \n",
    "    #Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test, yhat, labels=[1,0]), display_labels=[1,0])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'{title}\\n Accuracy: {acc}; F1: {fscore}; APS: {apscore}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a8eaf",
   "metadata": {},
   "source": [
    "## Gridsearch\n",
    "\n",
    "We'll gridsearch over hyperparameters for all three models and compare performance; given that we established earlier that the class imbalance isn't causing too many problems, we'll use the area under the ROC curve as an appropriate metric for performance evaluation.\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "We define a parameter grid, tune the hyperparameters and output the confusion matrix and scores to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessing', col_transform), ('lr', LogisticRegression())])\n",
    "\n",
    "C = np.logspace(-3, 2, 10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['lbfgs', 'liblinear', 'sag', 'saga']\n",
    "max_iter = [500, 700, 1000, 1200]\n",
    "class_weight = [{0:10,1:1}, {0:1,1:1}, {0:7,1:3}]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(lr__C=C, \n",
    "            lr__penalty=penalty, \n",
    "            lr__solver=solver,\n",
    "            lr__max_iter=max_iter,\n",
    "            lr__class_weight=class_weight)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline, \n",
    "                           param_grid=grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=cv, \n",
    "                           scoring='roc_auc',\n",
    "                           verbose=2)\n",
    "\n",
    "grid_result_lr = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(f\"Best: {grid_result_lr.best_score_} using {grid_result_lr.best_params_}\")\n",
    "print(f'Best estimator: {grid_result_lr.best_estimator_}')\n",
    "means = grid_result_lr.cv_results_['mean_test_score']\n",
    "stds = grid_result_lr.cv_results_['std_test_score']\n",
    "params = grid_result_lr.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"{mean} ({stdev}) with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c165a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = grid_result_lr.best_estimator_\n",
    "\n",
    "yhat = lr_best.predict(X_test)\n",
    "acc = round(lr_best.score(X_test, y_test), 3)\n",
    "fscore = round(f1_score(y_test, yhat), 3)\n",
    "apscore = round(average_precision_score(y_test, yhat), 3)\n",
    "\n",
    "print(acc, fscore, apscore, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results_unbalanced.reset_index()\n",
    "x[x['model'] == 'LR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ae74b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716418ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessing', col_transform), ('rf', RandomForestClassifier())])\n",
    "\n",
    "bootstrap = [True]\n",
    "max_depth = [9, 10, 20]\n",
    "max_features = [2, 3, 4]\n",
    "min_samples_leaf = [3, 4, 5]\n",
    "min_samples_split = [8, 10, 12]\n",
    "n_estimators = [50, 100, 500]\n",
    "class_weight = [{0:10,1:1}, {0:1,1:1}, {0:2,1:1}, {0:7,1:3}]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(rf__bootstrap=bootstrap, \n",
    "            rf__max_depth=max_depth, \n",
    "            rf__max_features=max_features,\n",
    "            rf__min_samples_leaf=min_samples_leaf,\n",
    "            rf__min_samples_split=min_samples_split, \n",
    "            rf__n_estimators=n_estimators,\n",
    "            rf__class_weight=class_weight)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline, \n",
    "                           param_grid=grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=cv, \n",
    "                           scoring='roc_auc',\n",
    "                           verbose=2)\n",
    "\n",
    "grid_result_rf = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(f\"Best: {grid_result_rf.best_score_} using {grid_result_rf.best_params_}\")\n",
    "print(f'Best estimator: {grid_result_rf.best_estimator_}')\n",
    "means = grid_result_rf.cv_results_['mean_test_score']\n",
    "stds = grid_result_rf.cv_results_['std_test_score']\n",
    "params = grid_result_rf.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"{mean} ({stdev}) with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = grid_result_rf.best_estimator_\n",
    "\n",
    "yhat = rf_best.predict(X_test)\n",
    "acc = round(rf_best.score(X_test, y_test), 3)\n",
    "fscore = round(f1_score(y_test, yhat), 3)\n",
    "apscore = round(average_precision_score(y_test, yhat), 3)\n",
    "\n",
    "print(acc, fscore, apscore, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d80f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
