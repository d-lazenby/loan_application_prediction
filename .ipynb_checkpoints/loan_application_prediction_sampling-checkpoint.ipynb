{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bbbd1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "df = pd.read_csv('Loan-Approval-Prediction.csv')\n",
    "\n",
    "categorical = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', \n",
    "               'Property_Area']\n",
    "\n",
    "numerical = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe145f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            614 non-null    object \n",
      " 1   Gender             601 non-null    object \n",
      " 2   Married            611 non-null    object \n",
      " 3   Dependents         599 non-null    object \n",
      " 4   Education          614 non-null    object \n",
      " 5   Self_Employed      582 non-null    object \n",
      " 6   ApplicantIncome    614 non-null    int64  \n",
      " 7   CoapplicantIncome  614 non-null    float64\n",
      " 8   LoanAmount         592 non-null    float64\n",
      " 9   Loan_Amount_Term   600 non-null    float64\n",
      " 10  Credit_History     564 non-null    float64\n",
      " 11  Property_Area      614 non-null    object \n",
      " 12  Loan_Status        614 non-null    object \n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047dbc23",
   "metadata": {},
   "source": [
    "### Split into training and test sets\n",
    "\n",
    "Ideally, to avoid data leakage, no data from the test set should be used in preprocessing, so we'll split into training and test sets beforehand. We can drop the loan ID since this won't be used in modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b5d5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate target out and remove loan ID column\n",
    "X = df.iloc[:, 1:].copy()\n",
    "y = X.pop('Loan_Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94594185",
   "metadata": {},
   "source": [
    "We encode the target as a binary, applying the convention that the minority class is labelled as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc3d506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda x: 1 if x=='N' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f76fd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split into train and test sets and stratify target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69a08aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(429, 11), (185, 11), (429,), (185,)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shapes of splits\n",
    "[split.shape for split in [X_train, X_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f39ef266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.687646\n",
      "1    0.312354\n",
      "Name: Loan_Status, dtype: float64\n",
      "\n",
      "0    0.686486\n",
      "1    0.313514\n",
      "Name: Loan_Status, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check class distribution in target splits\n",
    "print(y_train.value_counts(normalize=True), y_test.value_counts(normalize=True), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b96970",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "\n",
    "- For the categorical columns (including the loan term), we'll replace all nulls with the most frequent value. Given that 8% of the credit history values are null, this isn't ideal, but they mostly (~84%) take the value 1, so this seems reasonable in this case.\n",
    "- Since the other numerical columns are right-skewed, we'll replace the nulls with the median rather than the mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d1116c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check nulls\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a1288064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values with median\n",
    "value = {'LoanAmount': X_train.loc[:, ('LoanAmount')].median()}\n",
    "X_train = X_train.fillna(value=value)\n",
    "X_test = X_test.fillna(value=value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0bba6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values with most frequent value\n",
    "for col in categorical + ['Loan_Amount_Term']:\n",
    "    value = {col: X_train.loc[:, (col)].value_counts().index[0]}\n",
    "    X_train = X_train.fillna(value=value)\n",
    "    X_test = X_test.fillna(value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e0f53474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "#Check for nulls in data\n",
    "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cf51b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First we'll recast our datatypes where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c83933be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender               object\n",
      "Married              object\n",
      "Dependents           object\n",
      "Education            object\n",
      "Self_Employed        object\n",
      "ApplicantIncome       int64\n",
      "CoapplicantIncome     int64\n",
      "LoanAmount            int64\n",
      "Loan_Amount_Term      int64\n",
      "Credit_History        int64\n",
      "Property_Area        object\n",
      "dtype: object \n",
      "\n",
      "Gender               object\n",
      "Married              object\n",
      "Dependents           object\n",
      "Education            object\n",
      "Self_Employed        object\n",
      "ApplicantIncome       int64\n",
      "CoapplicantIncome     int64\n",
      "LoanAmount            int64\n",
      "Loan_Amount_Term      int64\n",
      "Credit_History        int64\n",
      "Property_Area        object\n",
      "dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in X_train, X_test:\n",
    "    data = data.astype({'CoapplicantIncome': 'int64', \n",
    "                        'LoanAmount': 'int64', \n",
    "                        'Loan_Amount_Term': 'int64', \n",
    "                        'Credit_History': 'int64'})\n",
    "    print(data.dtypes, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cf3c5",
   "metadata": {},
   "source": [
    "We note the following with regard to encoding each feature. \n",
    "- Gender, Married, Education, Self_Employed and Credit_History are binary, so we'll one-hot encode these. Property_Area takes three values but there doesn't appear to be any inherent ordering so we'll also one-hot encode this and drop a redundant column.\n",
    "- Dependents takes string values 0, 1, 2 and 3+. There is an ordering here so we could cast these as integers 0, 1, 2, 3, respectively, but note that this doesn't take into account the fact that a label 3 could then indicate any number of dependents from 3 upwards, while the other labels exactly match the data. Given this, we'll simply one-hot encode these as well, dropping a redundant column, as is standard.\n",
    "- ApplicantIncome, CoapplicantIncome, LoanAmount and Loan_Amount_Term are all numeric, take only positive values, and are skewed, so we'll use a power transformer to make these more Gaussian and scale them using a MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2483b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b83c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical transforms\n",
    "num_transformer = Pipeline(steps=[('scaler1', StandardScaler()), \n",
    "                                  ('power', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "                                  ('scaler2', MinMaxScaler(feature_range=(0,1)))])\n",
    "\n",
    "#categorical transforms\n",
    "cat_transformer = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
    "\n",
    "#transformer\n",
    "col_transform = ColumnTransformer(transformers=[('categorical', cat_transformer, categorical),\n",
    "                                                ('numerical', num_transformer, numerical[:-1]),\n",
    "                                                ('scaler', MinMaxScaler(feature_range=(0,1)), [numerical[-1]])],\n",
    "                                  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d623ee",
   "metadata": {},
   "source": [
    "We'll fit a basic logistic regression model using the area under the receiver operating characteristic (AU-ROC) curve as a performance metric to obtain a baseline for comparison with various sampling techniques. AU-ROC can be overoptimistic when dealing with severely imbalanced classification problems where we have only few samples of the minority class, but that's not the case here and it should be fine for our present purposes. We use repeated stratified cross-validation with three repeats and five folds.\n",
    "\n",
    "Beforehand, we note that a no-skill classifier would have an AU-ROC of 0.5 (diagonal line across the AU-ROC plot) since this is equivalent to picking randomly in proportion to the class base rates; this value therefore gives us a floor from which to measure performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bf2fb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, cross_validate, cross_val_score\n",
    "from scikitplot.metrics import roc_curve, plot_roc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83cb24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=21)\n",
    "\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', \n",
    "                             cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fdcd9986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced Training AU-ROC: 0.761152 (0.04)\n",
      "Balanced Training AU-ROC: 0.763884 (0.04)\n"
     ]
    }
   ],
   "source": [
    "weights = [('Unbalanced', None), ('Balanced', 'balanced')]\n",
    "\n",
    "for weight in weights: \n",
    "    pipeline = Pipeline(steps=[('preprocessing', col_transform), ('LR', LogisticRegression(class_weight=weight[1]))])\n",
    "\n",
    "    scores = run_model(pipeline, X_train, y_train)\n",
    "\n",
    "    print(f'{weight[0]} Training AU-ROC: {round(np.mean(scores), 6)} ({round(np.std(scores), 2)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c461f15",
   "metadata": {},
   "source": [
    "A basic model results in 0.26 increase over a no-skill classifier and a slight advantage to using a cost-sensitive algorithm – not a bad start. We'll next explore engineering polynomial features for the numerical variables to see if we can improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96688e5f",
   "metadata": {},
   "source": [
    "### Polynomial features\n",
    "\n",
    "We'll initially create polynomial predictors of degree 3 and include interaction terms; we do this by modifying the numerical pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d1110cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bfda2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update num transforms\n",
    "num_transformer_pf = Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "                                  ('scaler', MinMaxScaler(feature_range=(0,1)))])\n",
    "\n",
    "#update transformer\n",
    "col_transform_pf = ColumnTransformer(transformers=[('categorical', cat_transformer, categorical),\n",
    "                                                ('numerical', num_transformer_pf, numerical[:-1]),\n",
    "                                                ('scaler', MinMaxScaler(feature_range=(0,1)), [numerical[-1]])],\n",
    "                                  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e331a71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AU-ROC: 0.760264 (0.05)\n"
     ]
    }
   ],
   "source": [
    "#Define pipeline\n",
    "pipeline_pf = Pipeline(steps=[('preprocessing', col_transform_pf), ('LR', LogisticRegression(class_weight=None))])\n",
    "\n",
    "scores = run_model(pipeline_pf, X_train, y_train)\n",
    "\n",
    "print(f'Training AU-ROC: {round(np.mean(scores), 6)} ({round(np.std(scores), 2)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c14a",
   "metadata": {},
   "source": [
    "This has made no real difference to our score – a slight drop, if anything. We'll try some sampling techniques next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61c822",
   "metadata": {},
   "source": [
    "### Simple sampling\n",
    "\n",
    "First we'll use simple over- and undersampling in the pipeline. Simple oversampling duplicates examples in the minority class and simple undersampling deletes examples in the majority class; both even out the class distribution but with undersampling we end up with a smaller training set. They can also be used together – this is as simple as including the strategies one after the other in the pipeline – and we'll also investigate this. Finally, we'll use SMOTE (synthetic minority oversampling technique), which synthesises new data points from the minority set to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "373063d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling and pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f887a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "under = RandomUnderSampler(random_state=21, sampling_strategy='majority')\n",
    "over = RandomOverSampler(random_state=21, sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "34756e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target after undersampling: Counter({0: 134, 1: 134})\n",
      "Target after oversampling: Counter({0: 295, 1: 295})\n"
     ]
    }
   ],
   "source": [
    "#Check class distributions after sampling\n",
    "from collections import Counter\n",
    "\n",
    "X_under, y_under = under.fit_resample(X_train, y_train)\n",
    "X_over, y_over = over.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Target after undersampling:', Counter(y_under))\n",
    "print('Target after oversampling:', Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "386385b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models, titles = [], []\n",
    "    \n",
    "    #Over\n",
    "    steps = [('Oversampling', over), ('preprocessing', col_transform), ('LR', LogisticRegression())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    titles.append('Oversampling')\n",
    "    \n",
    "    #Under\n",
    "    steps = [('Undersampling', under), ('preprocessing', col_transform), ('LR', LogisticRegression())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    titles.append('Undersampling')\n",
    "    \n",
    "    #Combined over/under\n",
    "    steps = [('Oversampling', over), ('Undersampling', under), ('preprocessing', col_transform), ('LR', LogisticRegression())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    titles.append('Over/undersampling')\n",
    "\n",
    "    #SMOTE\n",
    "    steps = [('preprocessing', col_transform), ('SMOTE', smote), ('LR', LogisticRegression())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    titles.append('SMOTE')\n",
    "    \n",
    "    #Combined under/SMOTE\n",
    "    steps = [('preprocessing', col_transform), ('Undersampling', under), ('SMOTE', smote), ('LR', LogisticRegression())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    titles.append('Undersampling/SMOTE')\n",
    "    \n",
    "    return models, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4f4407cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Oversampling training AU-ROC: 0.756693547 (0.04) \n",
      "\n",
      "— Undersampling training AU-ROC: 0.754341913 (0.05) \n",
      "\n",
      "— Over/undersampling training AU-ROC: 0.756693547 (0.04) \n",
      "\n",
      "— SMOTE training AU-ROC: 0.720667343 (0.04) \n",
      "\n",
      "— Undersampling/SMOTE training AU-ROC: 0.754465852 (0.05) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models, titles = get_models()\n",
    "\n",
    "for model, title in zip(models, titles):\n",
    "    scores = run_model(model, X_train, y_train)\n",
    "    print(f'— {title} training AU-ROC: {round(np.mean(scores), 9)} ({round(np.std(scores), 2)})', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b336175",
   "metadata": {},
   "source": [
    "Simple random oversampling appears to perform slightly better than both undersampling and SMOTE, which is surprising given that SMOTE is a well-used sampling technique that is known to be effective. Combining undersampling with SMOTE gives better results, though still slightly lower than just oversampling alone. We can modify the default sampling parameters in SMOTE to investigate whether a different number of nearest neighbours or sampling strategy could help with performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c61d57a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbours: 1; strategy (SMOTE): 0.6; strategy (under): 0.5; ROC AUC: 0.757\n",
      "Neighbours: 4; strategy (SMOTE): minority; strategy (under): 0.9; ROC AUC: 0.756\n",
      "Neighbours: 4; strategy (SMOTE): 0.6; strategy (under): 0.5; ROC AUC: 0.756\n",
      "Neighbours: 4; strategy (SMOTE): 0.7; strategy (under): 0.5; ROC AUC: 0.755\n",
      "Neighbours: 1; strategy (SMOTE): minority; strategy (under): 0.9; ROC AUC: 0.755\n",
      "Neighbours: 2; strategy (SMOTE): 0.6; strategy (under): 0.5; ROC AUC: 0.755\n"
     ]
    }
   ],
   "source": [
    "ks = [*range(1,11)]\n",
    "samps1 = [0.5, 0.6, 0.7, 0.8, 0.9, 'minority']\n",
    "samps2 = [0.5, 0.6, 0.7, 0.8, 0.9, 'majority']\n",
    "roc_aucs = []\n",
    "for samp1 in samps1:\n",
    "    for samp2 in samps2:\n",
    "        for k in ks:\n",
    "            try:\n",
    "                smote = SMOTENC(sampling_strategy=samp1, k_neighbors=k, \n",
    "                                categorical_features=[*range(7)], random_state=21)\n",
    "\n",
    "                under = RandomUnderSampler(sampling_strategy=samp2, random_state=21)\n",
    "\n",
    "                #Define pipeline for combined sampling\n",
    "                pipeline = Pipeline(steps=[('preprocessing', col_transform), \n",
    "                                           ('Undersampling', under),\n",
    "                                           ('smote', smote),\n",
    "                                           ('LR', LogisticRegression())])\n",
    "\n",
    "                scores = run_model(pipeline, X_train, y_train)\n",
    "                roc_aucs.append((k, samp1, samp2, np.mean(scores), np.std(scores)))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "#Print top ROC combinations\n",
    "roc_aucs.sort(key=lambda x: x[3], reverse=True)\n",
    "for score in roc_aucs[:6]:\n",
    "    print(f'Neighbours: {score[0]}; strategy (SMOTE): {score[1]}; strategy (under): {score[2]}; ROC AUC: {score[3]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3302a3",
   "metadata": {},
   "source": [
    "It appears that the number of neighbours makes little difference but a combination of sampling strategies of 0.5–0.8 produces better results. We note that this is still only on a par with simple oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9ad28",
   "metadata": {},
   "source": [
    "Using sampling and polynomial features we haven't managed to increase performance from our basic model, which though disappointing is a still 0.26 better than a naive classifier. We'll gridsearch through a parameter space to obtain our final optimized model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4235",
   "metadata": {},
   "source": [
    "### Gridsearch Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e98e5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "afc1eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 640 candidates, totalling 9600 fits\n",
      "Best: 0.7675374635826613 using {'lr__C': 0.001, 'lr__class_weight': 'balanced', 'lr__max_iter': 700, 'lr__penalty': 'l2', 'lr__solver': 'sag'}\n",
      "Best estimator: Pipeline(steps=[('preprocessing',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('categorical',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['Gender', 'Married',\n",
      "                                                   'Dependents', 'Education',\n",
      "                                                   'Self_Employed',\n",
      "                                                   'Credit_History',\n",
      "                                                   'Property_Area']),\n",
      "                                                 ('numerical',\n",
      "                                                  Pipeline(steps=[('scaler1',\n",
      "                                                                   StandardScaler()),\n",
      "                                                                  ('power',\n",
      "                                                                   PowerTransformer(standardize=False)),\n",
      "                                                                  ('scaler2',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  ['ApplicantIncome',\n",
      "                                                   'CoapplicantIncome',\n",
      "                                                   'LoanAmount']),\n",
      "                                                 ('scaler', MinMaxScaler(),\n",
      "                                                  ['Loan_Amount_Term'])])),\n",
      "                ('over',\n",
      "                 RandomOverSampler(random_state=21,\n",
      "                                   sampling_strategy='minority')),\n",
      "                ('lr',\n",
      "                 LogisticRegression(C=0.001, class_weight='balanced',\n",
      "                                    max_iter=700, solver='sag'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.5               nan 0.5        0.76749561 0.76748435\n",
      " 0.76741191 0.76745376        nan 0.5               nan 0.5\n",
      " 0.76749561 0.76748435 0.76753746 0.76749561        nan 0.5\n",
      "        nan 0.5        0.76749561 0.76748435 0.76741191 0.76741191\n",
      "        nan 0.5               nan 0.5        0.76749561 0.76748435\n",
      " 0.76745376 0.76745376        nan 0.5               nan 0.5\n",
      " 0.76749561 0.76748435 0.76745376 0.76749561        nan 0.5\n",
      "        nan 0.5        0.76749561 0.76748435 0.76749722 0.76745376\n",
      "        nan 0.5               nan 0.5        0.76749561 0.76748435\n",
      " 0.76753746 0.76745376        nan 0.5               nan 0.5\n",
      " 0.76749561 0.76748435 0.76741191 0.76749561        nan 0.5\n",
      "        nan 0.5        0.76706746 0.76632382 0.76710931 0.76710931\n",
      "        nan 0.5               nan 0.5        0.76706746 0.76632382\n",
      " 0.76710931 0.76706746        nan 0.5               nan 0.5\n",
      " 0.76706746 0.76632382 0.76706746 0.76706746        nan 0.5\n",
      "        nan 0.5        0.76706746 0.76632382 0.76706746 0.76706746\n",
      "        nan 0.5               nan 0.5        0.76706746 0.76632382\n",
      " 0.76710931 0.76706746        nan 0.5               nan 0.5\n",
      " 0.76706746 0.76632382 0.76706746 0.76715116        nan 0.5\n",
      "        nan 0.5        0.76706746 0.76632382 0.76706746 0.76706746\n",
      "        nan 0.5               nan 0.5        0.76706746 0.76632382\n",
      " 0.76706746 0.76706746        nan 0.5               nan 0.5\n",
      " 0.7663866  0.76638499 0.7663866  0.7663866         nan 0.5\n",
      "        nan 0.5        0.7663866  0.76638499 0.7663866  0.7663866\n",
      "        nan 0.5               nan 0.5        0.7663866  0.76638499\n",
      " 0.7663866  0.7663866         nan 0.5               nan 0.5\n",
      " 0.7663866  0.76638499 0.7663866  0.7663866         nan 0.5\n",
      "        nan 0.5        0.7663866  0.76638499 0.7663866  0.7663866\n",
      "        nan 0.5               nan 0.5        0.7663866  0.76638499\n",
      " 0.7663866  0.7663866         nan 0.5               nan 0.5\n",
      " 0.7663866  0.76638499 0.7663866  0.7663866         nan 0.5\n",
      "        nan 0.5        0.7663866  0.76638499 0.7663866  0.76634475\n",
      "        nan 0.70437491        nan 0.70437491 0.76469651 0.76517295\n",
      " 0.76469651 0.76465466        nan 0.70437491        nan 0.70437491\n",
      " 0.76469651 0.76517295 0.76465466 0.76465466        nan 0.70437491\n",
      "        nan 0.70437491 0.76469651 0.76517295 0.76465466 0.76465466\n",
      "        nan 0.70437491        nan 0.70437491 0.76469651 0.76517295\n",
      " 0.76469651 0.76469651        nan 0.70437491        nan 0.70437491\n",
      " 0.76469651 0.76517295 0.76465466 0.76465466        nan 0.70437491\n",
      "        nan 0.70437491 0.76469651 0.76517295 0.76469651 0.76465466\n",
      "        nan 0.70437491        nan 0.70437491 0.76469651 0.76517295\n",
      " 0.76465466 0.76465466        nan 0.70437491        nan 0.70437491\n",
      " 0.76469651 0.76517295 0.76469651 0.76469651        nan 0.74995815\n",
      "        nan 0.74995815 0.76089623 0.76141291 0.76089623 0.76089784\n",
      "        nan 0.7500008         nan 0.74926763 0.76089623 0.76141291\n",
      " 0.76089623 0.76089784        nan 0.74993723        nan 0.74991711\n",
      " 0.76089623 0.76141291 0.76089623 0.76089784        nan 0.74993723\n",
      "        nan 0.74997988 0.76089623 0.76141291 0.76089623 0.76089784\n",
      "        nan 0.74995896        nan 0.74997988 0.76089623 0.76141291\n",
      " 0.76089623 0.76089784        nan 0.74993723        nan 0.74995735\n",
      " 0.76089623 0.76141291 0.76089623 0.76089784        nan 0.74993642\n",
      "        nan 0.74995815 0.76089623 0.76141291 0.76089623 0.76089784\n",
      "        nan 0.75              nan 0.74997908 0.76089623 0.76141291\n",
      " 0.76089623 0.76089784        nan 0.75592737        nan 0.75584368\n",
      " 0.75777681 0.75782832 0.75781866 0.75786051        nan 0.75584207\n",
      "        nan 0.7554461  0.75777681 0.75782832 0.75781866 0.75786051\n",
      "        nan 0.75584207        nan 0.75588552 0.75777681 0.75782832\n",
      " 0.75781866 0.75786051        nan 0.75590484        nan 0.75584207\n",
      " 0.75777681 0.75782832 0.75781866 0.75786051        nan 0.75588392\n",
      "        nan 0.75588552 0.75777681 0.75782832 0.75781866 0.75786051\n",
      "        nan 0.75588392        nan 0.75584207 0.75777681 0.75782832\n",
      " 0.75781866 0.75786051        nan 0.75582034        nan 0.75592576\n",
      " 0.75777681 0.75782832 0.75781866 0.75786051        nan 0.75586219\n",
      "        nan 0.75584207 0.75777681 0.75782832 0.75781866 0.75786051\n",
      "        nan 0.75302365        nan 0.7529818  0.75438698 0.75447712\n",
      " 0.75438698 0.75442883        nan 0.75302365        nan 0.7529818\n",
      " 0.75438698 0.75447712 0.75438698 0.75442883        nan 0.75302365\n",
      "        nan 0.7529818  0.75438698 0.75447712 0.75438698 0.75442883\n",
      "        nan 0.75293995        nan 0.7529818  0.75438698 0.75447712\n",
      " 0.75438698 0.75442883        nan 0.75293995        nan 0.7529818\n",
      " 0.75438698 0.75447712 0.75438698 0.75442883        nan 0.75302365\n",
      "        nan 0.7529818  0.75438698 0.75447712 0.75438698 0.75442883\n",
      "        nan 0.75302365        nan 0.7529818  0.75438698 0.75447712\n",
      " 0.75438698 0.75442883        nan 0.7529818         nan 0.7529818\n",
      " 0.75438698 0.75447712 0.75438698 0.75442883        nan 0.75257617\n",
      "        nan 0.75257617 0.75300755 0.75304457 0.75300594 0.75296248\n",
      "        nan 0.75261802        nan 0.75257617 0.75300755 0.75304457\n",
      " 0.75300594 0.75296248        nan 0.75261802        nan 0.75257617\n",
      " 0.75300755 0.75304457 0.75300594 0.75296248        nan 0.75257617\n",
      "        nan 0.75257617 0.75300755 0.75304457 0.75300594 0.75296248\n",
      "        nan 0.75261802        nan 0.75257617 0.75300755 0.75304457\n",
      " 0.75300594 0.75296248        nan 0.75261802        nan 0.75257617\n",
      " 0.75300755 0.75304457 0.75300594 0.75296248        nan 0.75257617\n",
      "        nan 0.75257617 0.75300755 0.75304457 0.75300594 0.75296248\n",
      "        nan 0.75257617        nan 0.75257617 0.75300755 0.75304457\n",
      " 0.75300594 0.75296248        nan 0.75287395        nan 0.75279025\n",
      " 0.7531669  0.75325382 0.75320875 0.75321036        nan 0.75287395\n",
      "        nan 0.75279025 0.7531669  0.75325382 0.75320875 0.75321036\n",
      "        nan 0.75278864        nan 0.75279025 0.7531669  0.75325382\n",
      " 0.75320875 0.75321036        nan 0.7529158         nan 0.75279025\n",
      " 0.7531669  0.75325382 0.75320875 0.75321036        nan 0.75287395\n",
      "        nan 0.75279025 0.7531669  0.75325382 0.75320875 0.75321036\n",
      "        nan 0.7528321         nan 0.75279025 0.7531669  0.75325382\n",
      " 0.75320875 0.75321036        nan 0.75279025        nan 0.75279025\n",
      " 0.7531669  0.75325382 0.75320875 0.75321036        nan 0.75283049\n",
      "        nan 0.75279025 0.7531669  0.75325382 0.75320875 0.75321036\n",
      "        nan 0.75304296        nan 0.75304296 0.75308481 0.75308481\n",
      " 0.75308481 0.7530832         nan 0.75304296        nan 0.75304296\n",
      " 0.75308481 0.75308481 0.75312666 0.75316851        nan 0.75304296\n",
      "        nan 0.75304296 0.75308481 0.75308481 0.75308481 0.75316851\n",
      "        nan 0.75304296        nan 0.75304296 0.75308481 0.75308481\n",
      " 0.75312666 0.75316851        nan 0.75304296        nan 0.75304296\n",
      " 0.75308481 0.75308481 0.75312666 0.75312505        nan 0.75304296\n",
      "        nan 0.75304296 0.75308481 0.75308481 0.75312666 0.75312666\n",
      "        nan 0.75304296        nan 0.75304296 0.75308481 0.75308481\n",
      " 0.75312666 0.75312505        nan 0.75304296        nan 0.75304296\n",
      " 0.75308481 0.75308481 0.75308481 0.75312505]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "C = np.logspace(-3, 2, 10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['lbfgs', 'liblinear', 'sag', 'saga']\n",
    "max_iter = [500, 700, 1000, 1200]\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy='minority', random_state=21)\n",
    "\n",
    "#Define pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessing', col_transform), \n",
    "                           ('over', over),\n",
    "                           ('lr', LogisticRegression())])\n",
    "\n",
    "# define grid search\n",
    "grid = dict(lr__C=C, \n",
    "            lr__penalty=penalty, \n",
    "            lr__solver=solver,\n",
    "            lr__max_iter=max_iter,\n",
    "            lr__class_weight=class_weight)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=21)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline, \n",
    "                           param_grid=grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=cv, \n",
    "                           scoring='roc_auc',\n",
    "                           verbose=2)\n",
    "\n",
    "grid_result_lr = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(f\"Best: {grid_result_lr.best_score_} using {grid_result_lr.best_params_}\")\n",
    "print(f'Best estimator: {grid_result_lr.best_estimator_}')\n",
    "means = grid_result_lr.cv_results_['mean_test_score']\n",
    "stds = grid_result_lr.cv_results_['std_test_score']\n",
    "params = grid_result_lr.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(f\"{mean} ({stdev}) with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eeb1fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AU-ROC score: 0.7303828400760249\n"
     ]
    }
   ],
   "source": [
    "lr_best = grid_result_lr.best_estimator_\n",
    "yhat_prob = lr_best.predict_proba(X_test)\n",
    "\n",
    "print(f'Test AU-ROC score: {roc_auc_score(y_test, yhat_prob[:,1])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b848d9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_best.jlib']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(lr_best, 'lr_best.jlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c3b9d",
   "metadata": {},
   "source": [
    "We expect a drop in performance from the training and test set, and the observed decrease of about 0.04 seems okay, i.e. there's no need to be worried about overfitting to the training set. \n",
    "\n",
    "Ultimately, neither polynomial features nor sampling increased performance in this case from just a basic model. It may be that other models such as Random Forest might work better on this dataset than Logistic Regression so one next step could be to test a suite of models and to compare results. It may also be that with some domain knowledge new features could be engineered that might help to increase performance. One final note is that there was notable deviation in the scores when the random seed was changed, which may indicate that our dataset is too small for random fluctuations between samples to be smoothed out during cross-validation and sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722697e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
